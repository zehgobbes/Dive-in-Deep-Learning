{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c99c94f9-c4f7-4b7e-98b1-8076e9df8dd8",
   "metadata": {},
   "source": [
    "# Cap 2 - Álgebra Linear\n",
    "\n",
    "Escalares, vetores, matrizes e tensores são os objetos matemáticos básicos usados álgebra linear e possuem zero,\n",
    "um, dois e um número arbitrário de eixos, respectivamente.\n",
    "\n",
    "Tensores podem ter qualquer quantidade de eixos. É um 'vetor/array' com número arbitrário de eixos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d04430c9-15c9-44f3-b6c4-9f419150a447",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1],\n",
       "        [2, 3],\n",
       "        [4, 5]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# -------------------------------------------------------------------------------\n",
    "# 2.3.1. Scalars\n",
    "# Scalars are implemented as tensors that contain only one element.\n",
    "x = torch.tensor(3.0)\n",
    "y = torch.tensor(2.0)\n",
    "\n",
    "x + y, x * y, x / y, x**y\n",
    "\n",
    "# -------------------------------------------------------------------------------\n",
    "# 2.3.2. Vectors\n",
    "# We denote vectors by bold lowercase letters.\n",
    "# Vectors are implemented as 1st-order tensors.\n",
    "# Caution: in Python, as in most programming languages, vector indices start at\n",
    "# , also known as zero-based indexing, whereas in linear algebra subscripts begin at\n",
    "#  (one-based indexing).\n",
    "\n",
    "x1 = torch.arange(3)\n",
    "x1\n",
    "x1[2]\n",
    "len(x1) # dimensionality of the vector = qtd of elements. Or\n",
    "x1.shape\n",
    "# To avoid confusion, we use order to refer to the number of axes and dimensionality exclusively to refer to the number of components.\n",
    "\n",
    "# -------------------------------------------------------------------------------\n",
    "# 2.3.3. Matrices\n",
    "# Scalar: 0th order\n",
    "# Tensor/vector: 1st order\n",
    "# Tensor/matrices: 2nd order\n",
    "# We denote matrices by bold CAPITAL letters\n",
    "\n",
    "A = torch.arange(6).reshape(3, 2)\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c839a83b-e08d-48a4-98ae-ad23630774c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1],\n",
       "        [2],\n",
       "        [3]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In code, we can access any matrix’s transpose as follows:\n",
    "A.T   # aplicado apenas em matrizes e não em vetores.\n",
    "# D1 e D2 são diferentes. A transposta de D1 não é calculada corretamente, já a de D2 sim.\n",
    "D1 = torch.tensor([1, 2, 3])  # Tensor:(3,)\n",
    "D2 = torch.tensor([[1, 2, 3]]) # Tensor:(3,1)\n",
    "D1\n",
    "# Out[41]: tensor([1, 2, 3])\n",
    "D2\n",
    "# Out[42]: tensor([[1, 2, 3]])\n",
    "D1.T\n",
    "# Out[43]: tensor([1, 2, 3])\n",
    "D2.T\n",
    "# Out[44]:\n",
    "# tensor([[1],\n",
    "#         [2],\n",
    "#         [3]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363bf904-9a7e-496a-92c2-a0c6385da3a6",
   "metadata": {},
   "source": [
    "# 2.3.4. Tensors\n",
    "\n",
    "Embora você possa ir longe em sua jornada de aprendizado de máquina apenas com escalares, vetores e matrizes, eventualmente poderá precisar\n",
    "trabalhar com tensores de ordem superior.\n",
    "Os tensores se tornarão mais importantes quando começarmos a trabalhar com imagens. Cada imagem chega como um 3rd\n",
    "Tensor de ordem com eixos correspondentes à altura, largura e canal. Em cada localização espacial, as intensidades de cada cor\n",
    "(vermelho, verde e azul) são empilhadas ao longo do canal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "164fda9d-6549-44f4-99e0-ac92c12d6daa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2,  3,  4,  5],\n",
       "         [ 6,  7,  8,  9],\n",
       "         [10, 11, 12, 13]],\n",
       "\n",
       "        [[14, 15, 16, 17],\n",
       "         [18, 19, 20, 21],\n",
       "         [22, 23, 24, 25]]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -------------------------------------------------------------------------------\n",
    "# 2.3.5. Basic Properties of Tensor Arithmetic\n",
    "\n",
    "A = torch.arange(6, dtype=torch.float32).reshape(2, 3)\n",
    "B = A.clone()  # Assign a copy of A to B by allocating new memory\n",
    "A, A + B\n",
    "\n",
    "# The elementwise product of two matrices\n",
    "A*B\n",
    "\n",
    "# Adding or multiplying a scalar and a tensor produces a result with the same shape as the original tensor.\n",
    "a = 2\n",
    "X = torch.arange(24).reshape(2, 3, 4)\n",
    "a + X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ccaf8d69-1b95-43a7-9467-a52d57b81728",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 4.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -------------------------------------------------------------------------------\n",
    "# 2.3.6. Reduction\n",
    "\n",
    "x = torch.arange(3, dtype=torch.float32)\n",
    "# tensor([0., 1., 2.])\n",
    "\n",
    "x.sum()\n",
    "# Out[8]: tensor(3.)\n",
    "\n",
    "A = torch.arange(6, dtype=torch.float32).reshape(2, 3)\n",
    "# tensor([[0., 1., 2.],\n",
    "#         [3., 4., 5.]])\n",
    "\n",
    "A.shape\n",
    "# Out[11]: torch.Size([2, 3])\n",
    "\n",
    "A.sum(axis=0) # Soma as colunas\n",
    "# tensor([3., 5., 7.])\n",
    "\n",
    "A.sum(axis=1) # soma as linhas\n",
    "# tensor([ 3., 12.])\n",
    "\n",
    "A.sum(axis=[0, 1])  # Soma todos os elementos da matriz\n",
    "# Out[16]: tensor(15.)\n",
    "\n",
    "A.numel()   # Quantidade de elementos na matriz\n",
    "# Out[17]: 6\n",
    "\n",
    "A.mean() # Média dos elementos da matriz\n",
    "# Out[18]: tensor(2.5000)   #A.sum() / A.numel()\n",
    "\n",
    "A.mean(axis=0) #Média de cada coluna\n",
    "# Out[20]: tensor([1.5000, 2.5000, 3.5000])\n",
    "\n",
    "A.mean(axis=1)  #Média de cada linha\n",
    "# Out[21]: tensor([1., 4.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7ce2e4a-c484-48db-a2a6-2fc6aff3030f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 3.],\n",
       "         [12.]]),\n",
       " torch.Size([2, 1]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -------------------------------------------------------------------------------\n",
    "# 2.3.7. Non-Reduction Sum\n",
    "\n",
    "sum_A = A.sum(axis=1, keepdims=True)  # mantem os resultados nas respectivas linhas\n",
    "sum_A, sum_A.shape\n",
    "# (tensor([[ 3.],\n",
    "#          [12.]]),\n",
    "#  torch.Size([2, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d2c08f-1a2a-4066-bbc0-e93206d5e5de",
   "metadata": {},
   "source": [
    "# 2.3.8. Produto escalar (Dot Products - also known as inner product) \n",
    "\n",
    "Resultado é uma grandeza escalar\n",
    "\n",
    "É a soma dos produtos dos elementos na mesma posição"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21a9e68a-0162-435a-a149-3c4d7b61c39c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 1., 2.]), tensor([1., 1., 1.]), tensor(3.))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.ones(3, dtype = torch.float32)\n",
    "x, y, torch.dot(x, y)    # O que é equivalente a fazer: torch.sum(x * y)\n",
    "# (tensor([0., 1., 2.]), tensor([1., 1., 1.]), tensor(3.))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7eba22b-8481-4762-a7cc-a673614d8f7a",
   "metadata": {},
   "source": [
    "# 2.3.9. Produtos matriciais e vetoriais \n",
    "\n",
    "Resutaldo é um vetor/matriz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6669b81d-2b2e-46a0-9896-97990ab7d83c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  7 -5]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 5., 14.])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Produto vetorial entre vetores utilizando numpy\n",
    "import numpy as np\n",
    "u = np.array([1,2,3])\n",
    "v = np.array([3,1,2])\n",
    "w = np.cross(u,v)\n",
    "print(w)\n",
    "# [ 1  7 -5]\n",
    "\n",
    "# Os produtos matriz-vetor também descrevem o cálculo principal envolvido no cálculo das saídas de cada camada em uma rede neural,\n",
    "# dadas as saídas da camada anterior.\n",
    "\n",
    "# Produto vetorial utilizando tensores:\n",
    "A.shape  # torch.Size([2, 3])    # 2 linhas e 3 colunas\n",
    "x.shape  # torch.Size([3])       # 1 linha e 3 colunas (ao invés de 3x1 como na matemática) - aqui deve coincidir a quantidade de colunas\n",
    "torch.mv(A, x)   # produto matriz(m) x vetor(v): mv  # tensor([ 5., 14.])\n",
    "A@x   # tensor([ 5., 14.])\n",
    "# A regra difere da matemática na maneira de ordenar a matriz e o vetor:\n",
    "# If input is a (nxm) tensor, vector is a 1-D tensor of size m, out will be 1-=D of size n.\n",
    "# No exemplo: Matriz A (2x3) e vetor x com m=3 elementos. Out será n=2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "abd5bd18-43c3-42c3-b2ca-7781762b16df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.,  3.,  3.,  3.],\n",
       "        [12., 12., 12., 12.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -------------------------------------------------------------------------------\n",
    "# 2.3.10. Matrix–Matrix Multiplication\n",
    "\n",
    "# Neste caso segue a regra convencional da matemática:\n",
    "# [A](2x3) x [B](3x4) = [C](2x4)\n",
    "# A já foi definido com 2 linhas e 3 colunas.\n",
    "B = torch.ones(3, 4) # matriz de 1s com 3 linhas e 4 colunas\n",
    "torch.mm(A, B) # produto entre matrizes.\n",
    "A@B # produto entre matrizes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891c64d9-6f70-4aca-98f5-f84e57f7e622",
   "metadata": {},
   "source": [
    "# 2.3.11. Norms (Norma ou Módulo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ba265bb-b068-452b-90bf-732480fe9731",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u = torch.tensor([3.0, -4.0])\n",
    "torch.norm(u)\n",
    "# tensor(5.)\n",
    "\n",
    "\n",
    "# The Frobenius norm behaves as if it were an l2 norm of a matrix-shaped vector.\n",
    "# Invoking the following function will calculate the Frobenius norm of a matrix.\n",
    "\n",
    "torch.norm(torch.ones((4, 9))) # matriz unitária de 4 linhas e 9 colunas, portanto, 36 elementos.\n",
    "# Somando todos os 1s temos 36, raiz quadrada de 36 é 6, a norma Frobenius da matriz.\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "62984435-b6a2-4256-a95a-970ba0dbc78f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True, True, True]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.tensor([[1,2,3,4,5]])\n",
    "a\n",
    "b = torch.tensor([1,2,3,4,5])\n",
    "b\n",
    "a==b\n",
    "# Apesar de terem os mesmo elementos e serem vetores com 5 colunas, certas operações não é possível fazer com b,\n",
    "# como por exemplo a transposta."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb3f19b-b8ce-45ac-95b1-f6bc84155871",
   "metadata": {},
   "source": [
    "# EXERCÍCIOS\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d2effef4-1bab-4fc2-ae70-82770c1d20b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True],\n",
       "        [True, True],\n",
       "        [True, True]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exercício 1\n",
    "import torch\n",
    "A = torch.arange(6).reshape(3, 2)\n",
    "B = A.T\n",
    "C = B.T\n",
    "C==A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8712991e-2bac-4546-b810-3ef8a0e6245d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True],\n",
       "        [True, True, True]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exercício 2\n",
    "A = torch.arange(6).reshape(3, 2)\n",
    "B = torch.tensor([[1, 4], [5, 6], [2, 7]])\n",
    "C = A + B\n",
    "D = C.T\n",
    "E = A.T + B.T\n",
    "D==E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4dffd7da-6d90-42d9-b6e8-942d32df095e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True],\n",
       "        [True, True, True],\n",
       "        [True, True, True]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exercício 3\n",
    "# Conhecemos como matriz simétrica aquela cuja matriz transposta é igual a ela mesma.\n",
    "B = torch.tensor([[1, 4, 1], [5, 6, 8], [2, 7, 3]])\n",
    "R=B+B.T\n",
    "RT = R.T\n",
    "R == RT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "301c59e3-65c7-4252-80ba-9f9a6f0d60d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Exercício 4\n",
    "X = torch.arange(24).reshape(2, 3, 4)\n",
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e0572313-e310-47ed-8feb-7057cd1ad2ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Exercício 5\n",
    "a = torch.tensor([[1],[2],[3]])\n",
    "a\n",
    "# Out[38]:\n",
    "# tensor([[1],\n",
    "#         [2],\n",
    "#         [3]])\n",
    "len(a)\n",
    "# Out[39]: 3\n",
    "# Corresponde sempre a primeira dimensão (1-D) ou axis=0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2aa4a24a-2ca0-4f4f-adcf-bff40a0210d5",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#Exercício 6\u001b[39;00m\n\u001b[0;32m      2\u001b[0m A\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# Somatório das linhas\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[43mA\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mA\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Erro\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Gera erro, pois está dividindo uma matriz 2x3 por um vetor 1x3\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "#Exercício 6\n",
    "A.sum(axis=1) # Somatório das linhas\n",
    "A / A.sum(axis=1) # Erro\n",
    "# Gera erro, pois está dividindo uma matriz 2x3 por um vetor 1x3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "62410f32-5e85-439c-a259-5d4f59b89ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercício 7\n",
    "# Não podemos percorrer na diagonal, o que seria a norma ou módulo entre os pontos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9bc54423-37bb-4544-85d6-c9b3ceec0e79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Exercício 8\n",
    "X = torch.arange(24).reshape(2, 3, 4)\n",
    "X.sum(axis = 0).shape # axis=0: somatório de ELEMENTO (escalar) a ELEMENTO equivalente # torch.Size([3, 4])\n",
    "X.sum(axis = 1).shape # axis=1: somatório dos elementos de cada COLUNA (eixo x)        # torch.Size([2, 4])\n",
    "X.sum(axis = 2).shape # axis=2: somatório dos elementos de cada LINHA (eixo y)         # torch.Size([2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0a2c015e-4420-464a-9bfa-482c4e070428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(65.7571)\n",
      "tensor(65.7571)\n"
     ]
    }
   ],
   "source": [
    "#Exercício 9\n",
    "Y = torch.arange(24).reshape(2, 3, 4)\n",
    "Y1 = Y.float()\n",
    "X = torch.linalg.norm(Y1)\n",
    "print(X)\n",
    "# Out[91]: tensor(65.7571)\n",
    "\n",
    "Y = torch.arange(24).reshape(2, 3, 4)\n",
    "Y1 = Y.float()\n",
    "sum = 0\n",
    "for i in Y1.flatten():\n",
    "    sum = sum + i**2\n",
    "print(sum.sqrt())\n",
    "# Portanto, soma o quadrado de todos os elementos dentro do tensor e obtém a raiz quadrada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab92edff-ec08-46fd-946c-b0fc1cfdcf65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Exercício 10\n",
    "# Acredito que (AB)C gera custa computacional muito maior do que A(BC), pois o primeiro produto é da ordem de 2^47 enquanto o segundo\n",
    "# produto é da ordem de 2^40 --> ERRADO. De acordo com a simulação realizada A(BC) gasta mais memória para ser realizada.\n",
    "# A = torch.randn( (2 ** 10, 2 ** 16) )\n",
    "# B = torch.randn( (2 ** 16, 2 ** 5) )\n",
    "# C = torch.randn( (2 ** 5, 2 ** 14) )\n",
    "\n",
    "# # (AB)C: 175 ms ± 1.54 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
    "# %%timeit          # exclusivo do jupyter notebook\n",
    "# p1 = A @ B\n",
    "# p1@C\n",
    "\n",
    "# # A(BC): DefaultCPUAllocator: not enough memory\n",
    "# %%timeit         # exclusivo do jupyter notebook\n",
    "# p1 = B @ C\n",
    "# A@p1\n",
    "\n",
    "# -------------------------------\n",
    "# OPÇÃO 2:\n",
    "import torch\n",
    "import time\n",
    "\n",
    "A = torch.randn([2**10,2**16])\n",
    "B = torch.randn([2**16,2**5])\n",
    "C = torch.randn([2**5,2**14])\n",
    "start = time.time()\n",
    "p1 = A @ B\n",
    "p1@C\n",
    "end = time.time()\n",
    "delay0 = end-start\n",
    "# RESULTADO: delay0 = 0.07573723793029785\n",
    "\n",
    "start = time.time()\n",
    "p1 = B @ C\n",
    "A@p1\n",
    "end = time.time()\n",
    "delay1 = end-start\n",
    "# RESULTADO: DefaultCPUAllocator: not enough memory: you tried to allocate 17179869184 bytes\n",
    "# -------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01b75bda-23d3-4c4f-b12b-fbc24f49b7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Exercício 11\n",
    "import torch\n",
    "import time\n",
    "A = torch.randn( (2 ** 10, 2 ** 16) )\n",
    "B = torch.randn( (2 ** 16, 2 ** 5) )\n",
    "C = torch.randn( (2 ** 5, 2 ** 16) )\n",
    "# Levando em consideração que a operação para a obtenção da matriz transposta já foi realizada anteriormente,\n",
    "# acredito que ambas possuem o mesmo custo computacional pois geram matrizes da ordem de 2^47.\n",
    "\n",
    "start = time.time()\n",
    "p1 = A @ B\n",
    "end = time.time()\n",
    "delay1 = end-start  # 0.08734321594238281\n",
    "\n",
    "D = C.T  # Desconsiderando o tempo que gasta para a operação de transposição.\n",
    "\n",
    "start = time.time()\n",
    "p1 = A @ D\n",
    "end = time.time()\n",
    "delay2 = end-start  # 0.05059695243835449\n",
    "# RESPOSTA: O tempo gasto é praticamente o mesmo, caso desconsideramos o tempo gasto para a operação de transposição."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b0eaafa-a11c-4560-935c-b7c8541d106f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True,  ..., True, True, True],\n",
       "        [True, True, True,  ..., True, True, True],\n",
       "        [True, True, True,  ..., True, True, True],\n",
       "        ...,\n",
       "        [True, True, True,  ..., True, True, True],\n",
       "        [True, True, True,  ..., True, True, True],\n",
       "        [True, True, True,  ..., True, True, True]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# #Exercício 12\n",
    "import torch\n",
    "import time\n",
    "A = torch.randn( [100, 200] )\n",
    "B = torch.randn( [100, 200] )\n",
    "C = torch.randn( [100, 200] )\n",
    "D = torch.stack([A,B,C]) # Empilhando os 3 tensores\n",
    "D.shape       # Out[27]: torch.Size([3, 100, 200])\n",
    "D[1] == B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff51fb2e-b603-4706-a7e9-a249d1d4eca9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
